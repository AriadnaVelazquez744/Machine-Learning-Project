{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e9c8cb",
   "metadata": {},
   "source": [
    "# Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8143f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading folds...\n",
      "Loaded fold 1: (8000, 10)\n",
      "Loaded fold 2: (8000, 10)\n",
      "Loaded fold 3: (8000, 10)\n",
      "Loaded fold 4: (8000, 10)\n",
      "Loaded fold 5: (8000, 10)\n",
      "Loaded fold 6: (8000, 10)\n",
      "Loaded fold 7: (8000, 10)\n",
      "Loaded fold 8: (8000, 10)\n",
      "Loaded fold 9: (8000, 10)\n",
      "Loaded fold 10: (8000, 10)\n",
      "\n",
      "Global variables initialized for 10-fold cross-validation\n",
      "Models to validate: ['SVM', 'XGBoost', 'RandomForest', 'NeuralNetwork']\n",
      "Results will be saved to: results\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import fbeta_score, f1_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # Special pipeline for sampling\n",
    "\n",
    "# Statistical Test Imports\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Global Constants\n",
    "N_FOLDS = 10\n",
    "RANDOM_STATE = 42\n",
    "BETA = 2  # For F2-score\n",
    "\n",
    "# Define paths\n",
    "FOLDS_DIR = '../dataset/folds/'\n",
    "STATS_DIR = 'results'\n",
    "MODELS_DIR = 'models/'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [STATS_DIR, MODELS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Define model names\n",
    "MODEL_NAMES = ['SVM', 'XGBoost', 'RandomForest', 'NeuralNetwork']\n",
    "\n",
    "# Data structures to store results\n",
    "@dataclass\n",
    "class FoldResults:\n",
    "    \"\"\"Store results from a single fold validation\"\"\"\n",
    "    fold_num: int\n",
    "    f2_score: float\n",
    "    f1_score: float  # Added for statistical comparison\n",
    "    accuracy: float  # Added for statistical comparison\n",
    "    precision: float\n",
    "    recall: float\n",
    "    conf_matrix: List[List[int]]  # [[TN, FP], [FN, TP]]\n",
    "    train_time: float\n",
    "    pred_time: float\n",
    "    train_samples: int\n",
    "    test_samples: int\n",
    "    y_true: List[float] = None  # Optional: true labels for McNemar test\n",
    "    y_pred: List[float] = None  # Optional: predictions for McNemar test\n",
    "\n",
    "@dataclass\n",
    "class ModelValidationResults:\n",
    "    \"\"\"Store all validation results for a single model\"\"\"\n",
    "    model_name: str\n",
    "    hyperparameters: Dict[str, Any]\n",
    "    fold_results: List[FoldResults]\n",
    "    \n",
    "    # Calculated metrics\n",
    "    f2_scores: List[float]\n",
    "    precision_scores: List[float]\n",
    "    recall_scores: List[float]\n",
    "    f1_scores: List[float] = None  # Added for statistical comparison\n",
    "    accuracy_scores: List[float] = None  # Added for statistical comparison\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize lists if not provided\"\"\"\n",
    "        if self.f1_scores is None:\n",
    "            self.f1_scores = [fr.f1_score for fr in self.fold_results]\n",
    "        if self.accuracy_scores is None:\n",
    "            self.accuracy_scores = [fr.accuracy for fr in self.fold_results]\n",
    "    \n",
    "    @property\n",
    "    def mean_f2(self) -> float:\n",
    "        return np.mean(self.f2_scores) if self.f2_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def std_f2(self) -> float:\n",
    "        return np.std(self.f2_scores) if self.f2_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def mean_f1(self) -> float:\n",
    "        return np.mean(self.f1_scores) if self.f1_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def std_f1(self) -> float:\n",
    "        return np.std(self.f1_scores) if self.f1_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def mean_accuracy(self) -> float:\n",
    "        return np.mean(self.accuracy_scores) if self.accuracy_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def std_accuracy(self) -> float:\n",
    "        return np.std(self.accuracy_scores) if self.accuracy_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def mean_precision(self) -> float:\n",
    "        return np.mean(self.precision_scores) if self.precision_scores else 0.0\n",
    "    \n",
    "    @property\n",
    "    def mean_recall(self) -> float:\n",
    "        return np.mean(self.recall_scores) if self.recall_scores else 0.0\n",
    "    \n",
    "    def calculate_confusion_matrices_sum(self) -> Tuple[int, int, int, int]:\n",
    "        \"\"\"Sum confusion matrices from all folds\"\"\"\n",
    "        total_tn = total_fp = total_fn = total_tp = 0\n",
    "        for fold_result in self.fold_results:\n",
    "            tn, fp, fn, tp = fold_result.conf_matrix[0][0], fold_result.conf_matrix[0][1], \\\n",
    "                            fold_result.conf_matrix[1][0], fold_result.conf_matrix[1][1]\n",
    "            total_tn += tn\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "            total_tp += tp\n",
    "        return total_tn, total_fp, total_fn, total_tp\n",
    "    \n",
    "    def save_to_file(self, filename: str = None):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        if filename is None:\n",
    "            filename = f\"{self.model_name}_validation_results.json\"\n",
    "        \n",
    "        filepath = os.path.join(STATS_DIR, filename)\n",
    "        \n",
    "        # Convert to dictionary\n",
    "        results_dict = {\n",
    "            'model_name': self.model_name,\n",
    "            'hyperparameters': self.hyperparameters,\n",
    "            'metrics_summary': {\n",
    "                'mean_f2': float(self.mean_f2),\n",
    "                'std_f2': float(self.std_f2),\n",
    "                'mean_f1': float(self.mean_f1),\n",
    "                'std_f1': float(self.std_f1),\n",
    "                'mean_accuracy': float(self.mean_accuracy),\n",
    "                'std_accuracy': float(self.std_accuracy),\n",
    "                'mean_precision': float(self.mean_precision),\n",
    "                'mean_recall': float(self.mean_recall),\n",
    "                'f2_scores': [float(score) for score in self.f2_scores],\n",
    "                'f1_scores': [float(score) for score in self.f1_scores],\n",
    "                'accuracy_scores': [float(score) for score in self.accuracy_scores],\n",
    "                'precision_scores': [float(score) for score in self.precision_scores],\n",
    "                'recall_scores': [float(score) for score in self.recall_scores],\n",
    "            },\n",
    "            'fold_details': [\n",
    "                {\n",
    "                    'fold_num': fr.fold_num,\n",
    "                    'f2_score': float(fr.f2_score),\n",
    "                    'f1_score': float(fr.f1_score),\n",
    "                    'accuracy': float(fr.accuracy),\n",
    "                    'precision': float(fr.precision),\n",
    "                    'recall': float(fr.recall),\n",
    "                    'conf_matrix': fr.conf_matrix,\n",
    "                    'train_time': float(fr.train_time),\n",
    "                    'pred_time': float(fr.pred_time),\n",
    "                    'train_samples': fr.train_samples,\n",
    "                    'test_samples': fr.test_samples,\n",
    "                    'y_true': (fr.y_true.tolist() if hasattr(fr.y_true, 'tolist') else list(fr.y_true)) if fr.y_true is not None else None,\n",
    "                    'y_pred': (fr.y_pred.tolist() if hasattr(fr.y_pred, 'tolist') else list(fr.y_pred)) if fr.y_pred is not None else None\n",
    "                }\n",
    "                for fr in self.fold_results\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results_dict, f, indent=2)\n",
    "        \n",
    "        print(f\"Results saved to {filepath}\")\n",
    "\n",
    "# Define hyperparameters for each model (from your report)\n",
    "MODEL_HYPERPARAMETERS = {\n",
    "    'SVM': {\n",
    "        'kernel': 'rbf',\n",
    "        'C': 69.84841896499474,\n",
    "        'gamma': 'auto',\n",
    "        'probability': True,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'class_weight': 'balanced'  # From final optimized SVM\n",
    "    },\n",
    "    \n",
    "    'XGBoost': {\n",
    "        'objective': 'binary:logistic',\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.0273,\n",
    "        'min_child_weight': 17,\n",
    "        'gamma': 3.459e-7,           # Lagrangian multiplier (min_split_loss)\n",
    "        'reg_lambda': 7.739,         # L2 regularization (λ)\n",
    "        'reg_alpha': 0.0067,         # L1 regularization (α)\n",
    "        'subsample': 0.997,\n",
    "        'colsample_bytree': 0.986,\n",
    "        'scale_pos_weight': 2.438,\n",
    "        'tree_method': 'hist',       # As used in the tuned XGBoost notebook\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'eval_metric': 'logloss'\n",
    "    },\n",
    "    \n",
    "    'RandomForest': {\n",
    "        'n_estimators': 179,\n",
    "        'max_depth': 19,\n",
    "        'min_samples_leaf': 3,\n",
    "        'random_state': RANDOM_STATE\n",
    "        # Sampling handled externally via RandomUnderSampler\n",
    "    },\n",
    "    \n",
    "    'NeuralNetwork': {\n",
    "        'hidden_layer_sizes': (32, 128),  # (32, 128) as in the best Optuna trial\n",
    "        'activation': 'relu',\n",
    "        'solver': 'adam',\n",
    "        'alpha': 1.249e-5,               # Regularization\n",
    "        'learning_rate_init': 0.00851,\n",
    "        'max_iter': 148,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': 0.1,\n",
    "        'n_iter_no_change': 10,\n",
    "        'batch_size': 'auto'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define sampling strategies for each model\n",
    "MODEL_SAMPLING_STRATEGIES = {\n",
    "    'SVM': None,           # Best SVM uses no external sampling\n",
    "    'XGBoost': None,       # XGBoost uses scale_pos_weight for class imbalance\n",
    "    'RandomForest': 'undersample',  # Random Under Sampling\n",
    "    'NeuralNetwork': 'smote'        # SMOTE oversampling\n",
    "}\n",
    "\n",
    "# Function to load folds\n",
    "def load_folds(folds_dir: str = FOLDS_DIR) -> List[pd.DataFrame]:\n",
    "    \"\"\"Load all fold CSVs into a list\"\"\"\n",
    "    folds = []\n",
    "    for i in range(1, N_FOLDS + 1):\n",
    "        fold_path = os.path.join(folds_dir, f'fold_{i}.csv')\n",
    "        if os.path.exists(fold_path):\n",
    "            fold_df = pd.read_csv(fold_path)\n",
    "            folds.append(fold_df)\n",
    "            print(f\"Loaded fold {i}: {fold_df.shape}\")\n",
    "        else:\n",
    "            print(f\"Warning: Fold {i} not found at {fold_path}\")\n",
    "            folds.append(pd.DataFrame())\n",
    "    return folds\n",
    "\n",
    "# Function to get train/test indices for a given test fold\n",
    "def get_fold_split(folds: List[pd.DataFrame], test_fold_idx: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Get training and testing data for a given fold index\"\"\"\n",
    "    test_data = folds[test_fold_idx]\n",
    "    \n",
    "    # Combine all other folds for training\n",
    "    train_folds = [folds[i] for i in range(len(folds)) if i != test_fold_idx]\n",
    "    train_data = pd.concat(train_folds, ignore_index=True)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# Function to split features and target\n",
    "def split_features_target(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Split data into features (X) and target (y)\"\"\"\n",
    "    # Assuming target column is 'Fault' based on your report\n",
    "    target_col = 'Fault'\n",
    "    \n",
    "    if target_col not in data.columns:\n",
    "        # Try to find it case-insensitively\n",
    "        possible_cols = [col for col in data.columns if col.lower() == 'fault']\n",
    "        if possible_cols:\n",
    "            target_col = possible_cols[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Target column 'Fault' not found. Available columns: {data.columns.tolist()}\")\n",
    "    \n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Function to calculate F2-score\n",
    "def calculate_f2_score(y_true, y_pred, beta: int = BETA) -> float:\n",
    "    \"\"\"Calculate F-beta score with given beta\"\"\"\n",
    "    return fbeta_score(y_true, y_pred, beta=beta)\n",
    "\n",
    "# Load all folds once (will be reused for all models)\n",
    "print(\"Loading folds...\")\n",
    "ALL_FOLDS = load_folds()\n",
    "\n",
    "# Verify folds were loaded correctly\n",
    "if len(ALL_FOLDS) == 0:\n",
    "    print(\"ERROR: No folds loaded. Check the folds directory.\")\n",
    "elif len(ALL_FOLDS) != N_FOLDS:\n",
    "    print(f\"WARNING: Expected {N_FOLDS} folds, but loaded {len(ALL_FOLDS)}\")\n",
    "    \n",
    "print(f\"\\nGlobal variables initialized for {N_FOLDS}-fold cross-validation\")\n",
    "print(f\"Models to validate: {MODEL_NAMES}\")\n",
    "print(f\"Results will be saved to: {STATS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3146b441",
   "metadata": {},
   "source": [
    "# SVM k=10 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f5797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to make one cycle of the validation of the SVM model depending on the fold number pass as param\n",
    "'''\n",
    "\n",
    "def validate_svm_fold(fold_idx: int):\n",
    "    \"\"\"\n",
    "    Validate SVM model for a single fold.\n",
    "    \n",
    "    Args:\n",
    "        fold_idx: Index of the fold to validate (0-based, 0-9 for 10 folds)\n",
    "    \n",
    "    This function:\n",
    "    - Validates the specified fold\n",
    "    - Loads existing results from file if available\n",
    "    - Updates or adds the fold result\n",
    "    - Saves all results back to file\n",
    "    \"\"\"\n",
    "    if fold_idx < 0 or fold_idx >= N_FOLDS:\n",
    "        raise ValueError(f\"fold_idx must be between 0 and {N_FOLDS-1}, got {fold_idx}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"SVM MODEL - FOLD {fold_idx + 1} VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get SVM hyperparameters and sampling strategy\n",
    "    svm_params = MODEL_HYPERPARAMETERS['SVM'].copy()\n",
    "    svm_sampling = MODEL_SAMPLING_STRATEGIES['SVM']\n",
    "    \n",
    "    # Add additional parameters used in training \n",
    "    svm_params['cache_size'] = 2000\n",
    "    svm_params['tol'] = 1e-3\n",
    "    \n",
    "    print(f\"\\nHyperparameters: {svm_params}\")\n",
    "    print(f\"Sampling strategy: {svm_sampling}\")\n",
    "    \n",
    "    # Get train/test split for this fold\n",
    "    train_data, test_data = get_fold_split(ALL_FOLDS, fold_idx)\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train, y_train = split_features_target(train_data)\n",
    "    X_test, y_test = split_features_target(test_data)\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "    print(f\"Train class distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"Test class distribution: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create SVM model with hyperparameters\n",
    "    svm_model = SVC(**svm_params)\n",
    "    \n",
    "    # Create pipeline: StandardScaler + SVM (no external sampling for SVM)\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', svm_model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    train_start = time.time()\n",
    "    svm_pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_start = time.time()\n",
    "    y_pred = svm_pipeline.predict(X_test)\n",
    "    pred_time = time.time() - pred_start\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f2 = calculate_f2_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Convert confusion matrix to list format [[TN, FP], [FN, TP]]\n",
    "    cm_list = [[int(cm[0, 0]), int(cm[0, 1])], [int(cm[1, 0]), int(cm[1, 1])]]\n",
    "    \n",
    "    # Store fold results (including predictions for McNemar test)\n",
    "    fold_result = FoldResults(\n",
    "        fold_num=fold_idx + 1,\n",
    "        f2_score=f2,\n",
    "        f1_score=f1,\n",
    "        accuracy=accuracy,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        conf_matrix=cm_list,\n",
    "        train_time=train_time,\n",
    "        pred_time=pred_time,\n",
    "        train_samples=len(X_train),\n",
    "        test_samples=len(X_test),\n",
    "        y_true=y_test.values if hasattr(y_test, 'values') else np.array(y_test),\n",
    "        y_pred=y_pred\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResults for Fold {fold_idx + 1}:\")\n",
    "    print(f\"F2-Score: {f2:.4f}, F1-Score: {f1:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    print(f\"Train time: {train_time:.2f}s, Prediction time: {pred_time:.4f}s\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "    \n",
    "    # Load existing results or create new\n",
    "    filename = \"SVM_validation_results.json\"\n",
    "    filepath = os.path.join(STATS_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        # Load existing results\n",
    "        with open(filepath, 'r') as f:\n",
    "            existing_data = json.load(f)\n",
    "        \n",
    "        # Extract existing fold results\n",
    "        existing_fold_details = existing_data.get('fold_details', [])\n",
    "        \n",
    "        # Remove the fold if it already exists (to update it)\n",
    "        existing_fold_details = [fd for fd in existing_fold_details if fd['fold_num'] != fold_idx + 1]\n",
    "        \n",
    "        # Add the new fold result\n",
    "        new_fold_detail = {\n",
    "            'fold_num': fold_result.fold_num,\n",
    "            'f2_score': float(fold_result.f2_score),\n",
    "            'f1_score': float(fold_result.f1_score),\n",
    "            'accuracy': float(fold_result.accuracy),\n",
    "            'precision': float(fold_result.precision),\n",
    "            'recall': float(fold_result.recall),\n",
    "            'conf_matrix': fold_result.conf_matrix,\n",
    "            'train_time': float(fold_result.train_time),\n",
    "            'pred_time': float(fold_result.pred_time),\n",
    "            'train_samples': fold_result.train_samples,\n",
    "            'test_samples': fold_result.test_samples,\n",
    "            'y_true': (fold_result.y_true.tolist() if hasattr(fold_result.y_true, 'tolist') else list(fold_result.y_true)) if fold_result.y_true is not None else None,\n",
    "            'y_pred': (fold_result.y_pred.tolist() if hasattr(fold_result.y_pred, 'tolist') else list(fold_result.y_pred)) if fold_result.y_pred is not None else None\n",
    "        }\n",
    "        existing_fold_details.append(new_fold_detail)\n",
    "        \n",
    "        # Reconstruct FoldResults objects from all folds\n",
    "        all_fold_results = []\n",
    "        for fd in existing_fold_details:\n",
    "            all_fold_results.append(FoldResults(\n",
    "                fold_num=fd['fold_num'],\n",
    "                f2_score=fd['f2_score'],\n",
    "                f1_score=fd['f1_score'],\n",
    "                accuracy=fd['accuracy'],\n",
    "                precision=fd['precision'],\n",
    "                recall=fd['recall'],\n",
    "                conf_matrix=fd['conf_matrix'],\n",
    "                train_time=fd['train_time'],\n",
    "                pred_time=fd['pred_time'],\n",
    "                train_samples=fd['train_samples'],\n",
    "                test_samples=fd['test_samples'],\n",
    "                y_true=np.array(fd['y_true']) if fd['y_true'] is not None else None,\n",
    "                y_pred=np.array(fd['y_pred']) if fd['y_pred'] is not None else None\n",
    "            ))\n",
    "        \n",
    "        # Extract all scores\n",
    "        all_f2_scores = [fr.f2_score for fr in all_fold_results]\n",
    "        all_f1_scores = [fr.f1_score for fr in all_fold_results]\n",
    "        all_accuracy_scores = [fr.accuracy for fr in all_fold_results]\n",
    "        all_precision_scores = [fr.precision for fr in all_fold_results]\n",
    "        all_recall_scores = [fr.recall for fr in all_fold_results]\n",
    "        \n",
    "        # Create ModelValidationResults with all folds\n",
    "        svm_validation_results = ModelValidationResults(\n",
    "            model_name='SVM',\n",
    "            hyperparameters=svm_params,\n",
    "            fold_results=all_fold_results,\n",
    "            f2_scores=all_f2_scores,\n",
    "            precision_scores=all_precision_scores,\n",
    "            recall_scores=all_recall_scores,\n",
    "            f1_scores=all_f1_scores,\n",
    "            accuracy_scores=all_accuracy_scores\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated existing results. Total folds validated: {len(all_fold_results)}/{N_FOLDS}\")\n",
    "    else:\n",
    "        # Create new results with just this fold\n",
    "        svm_validation_results = ModelValidationResults(\n",
    "            model_name='SVM',\n",
    "            hyperparameters=svm_params,\n",
    "            fold_results=[fold_result],\n",
    "            f2_scores=[f2],\n",
    "            precision_scores=[precision],\n",
    "            recall_scores=[recall],\n",
    "            f1_scores=[f1],\n",
    "            accuracy_scores=[accuracy]\n",
    "        )\n",
    "        print(f\"\\nCreated new results file. Folds validated: 1/{N_FOLDS}\")\n",
    "    \n",
    "    # Save results to file (using fixed filename so all folds save to same file)\n",
    "    svm_validation_results.save_to_file(filename)\n",
    "    \n",
    "    # Print summary if we have multiple folds\n",
    "    if len(svm_validation_results.fold_results) > 1:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CURRENT SVM CROSS-VALIDATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Folds completed: {len(svm_validation_results.fold_results)}/{N_FOLDS}\")\n",
    "        print(f\"Mean F2-Score: {svm_validation_results.mean_f2:.4f} ± {svm_validation_results.std_f2:.4f}\")\n",
    "        print(f\"Mean F1-Score: {svm_validation_results.mean_f1:.4f} ± {svm_validation_results.std_f1:.4f}\")\n",
    "        print(f\"Mean Accuracy: {svm_validation_results.mean_accuracy:.4f} ± {svm_validation_results.std_accuracy:.4f}\")\n",
    "        print(f\"Mean Precision: {svm_validation_results.mean_precision:.4f}\")\n",
    "        print(f\"Mean Recall: {svm_validation_results.mean_recall:.4f}\")\n",
    "        print(f\"\\nF2-Scores per fold: {[f'{s:.4f}' for s in svm_validation_results.f2_scores]}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1} validation completed. Results saved to {filepath}\")\n",
    "    return svm_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a6fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SVM MODEL - FOLD 1 VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Hyperparameters: {'kernel': 'rbf', 'C': 69.84841896499474, 'gamma': 'auto', 'probability': True, 'random_state': 42, 'class_weight': 'balanced', 'cache_size': 2000, 'tol': 0.001}\n",
      "Sampling strategy: None\n",
      "\n",
      "Fold 1/10\n",
      "----------------------------------------------------------------------\n",
      "Train samples: 72000, Test samples: 8000\n",
      "Train class distribution: {0.0: 49680, 1.0: 22320}\n",
      "Test class distribution: {0.0: 5520, 1.0: 2480}\n",
      "\n",
      "Results for Fold 1:\n",
      "F2-Score: 0.8809, F1-Score: 0.8594, Accuracy: 0.8718, Precision: 0.8471, Recall: 0.8874\n",
      "Train time: 3180.58s, Prediction time: 11.1922s\n",
      "Confusion Matrix:\n",
      "[[4671  849]\n",
      " [ 177 2303]]\n",
      "\n",
      "Created new results file. Folds validated: 1/10\n",
      "Results saved to results/SVM_validation_results.json\n",
      "\n",
      "Fold 1 validation completed. Results saved to results/SVM_validation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Manually determine the fold to validate\n",
    "# Change this variable to the fold index you want to validate (0-9 for 10 folds)\n",
    "# fold_idx is 0-based: 0 = fold 1, 1 = fold 2, ..., 9 = fold 10\n",
    "# Already processed folds: 1, \n",
    "FOLD_TO_VALIDATE = 1  # Change this value to validate different folds\n",
    "\n",
    "# Validate the specified fold\n",
    "svm_validation_results = validate_svm_fold(FOLD_TO_VALIDATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939c47a",
   "metadata": {},
   "source": [
    "# Random Forest k=10 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to make one cycle of the validation of the Random Forest model depending on the fold number pass as param\n",
    "'''\n",
    "def validate_rf_fold(fold_idx: int):\n",
    "    \"\"\"\n",
    "    Validate Random Forest model for a single fold.\n",
    "    \n",
    "    Args:\n",
    "        fold_idx: Index of the fold to validate (0-based, 0-9 for 10 folds)\n",
    "    \n",
    "    This function:\n",
    "    - Validates the specified fold\n",
    "    - Loads existing results from file if available\n",
    "    - Updates or adds the fold result\n",
    "    - Saves all results back to file\n",
    "    \"\"\"\n",
    "\n",
    "    if fold_idx < 0 or fold_idx >= N_FOLDS:\n",
    "        raise ValueError(f\"fold_idx must be between 0 and {N_FOLDS-1}, got {fold_idx}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"Random Forest MODEL - FOLD {fold_idx + 1} VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get Random Forest hyperparameters and sampling strategy\n",
    "    rf_params = MODEL_HYPERPARAMETERS['RandomForest'].copy()\n",
    "    rf_sampling = MODEL_SAMPLING_STRATEGIES['RandomForest']\n",
    "\n",
    "    print(f\"\\nHyperparameters: {rf_params}\")\n",
    "    print(f\"Sampling strategy: {rf_sampling}\")\n",
    "    \n",
    "    # Get train/test split for this fold\n",
    "    train_data, test_data = get_fold_split(ALL_FOLDS, fold_idx)\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train, y_train = split_features_target(train_data)\n",
    "    X_test, y_test = split_features_target(test_data)\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "    print(f\"Train class distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"Test class distribution: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create Random Forest model with hyperparameters\n",
    "    rf_model = RandomForestClassifier(**rf_params)\n",
    "    \n",
    "    # Create pipeline: RandomUnderSampler + Random Forest (external sampling for Random Forest)\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('sampler', RandomUnderSampler(random_state=RANDOM_STATE)),\n",
    "        ('model', rf_model)\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    train_start = time.time()\n",
    "    rf_pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_start = time.time()\n",
    "    y_pred = rf_pipeline.predict(X_test)\n",
    "    pred_time = time.time() - pred_start\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f2 = calculate_f2_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Convert confusion matrix to list format [[TN, FP], [FN, TP]]\n",
    "    cm_list = [[int(cm[0, 0]), int(cm[0, 1])], [int(cm[1, 0]), int(cm[1, 1])]]\n",
    "    \n",
    "    # Store fold results (including predictions for McNemar test)\n",
    "    fold_result = FoldResults(\n",
    "        fold_num=fold_idx + 1,\n",
    "        f2_score=f2,\n",
    "        f1_score=f1,\n",
    "        accuracy=accuracy,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        conf_matrix=cm_list,\n",
    "        train_time=train_time,\n",
    "        pred_time=pred_time,\n",
    "        train_samples=len(X_train),\n",
    "        test_samples=len(X_test),\n",
    "        y_true=y_test.values if hasattr(y_test, 'values') else np.array(y_test),\n",
    "        y_pred=y_pred\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResults for Fold {fold_idx + 1}:\")\n",
    "    print(f\"F2-Score: {f2:.4f}, F1-Score: {f1:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    print(f\"Train time: {train_time:.2f}s, Prediction time: {pred_time:.4f}s\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Load existing results or create new\n",
    "    filename = \"Random_Forest_validation_results.json\"\n",
    "    filepath = os.path.join(STATS_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        # Load existing results\n",
    "        with open(filepath, 'r') as f:\n",
    "            existing_data = json.load(f)\n",
    "        \n",
    "        # Extract existing fold results\n",
    "        existing_fold_details = existing_data.get('fold_details', [])\n",
    "        \n",
    "        # Remove the fold if it already exists (to update it)\n",
    "        existing_fold_details = [fd for fd in existing_fold_details if fd['fold_num'] != fold_idx + 1]\n",
    "        \n",
    "        # Add the new fold result\n",
    "        new_fold_detail = {\n",
    "            'fold_num': fold_result.fold_num,\n",
    "            'f2_score': float(fold_result.f2_score),\n",
    "            'f1_score': float(fold_result.f1_score),\n",
    "            'accuracy': float(fold_result.accuracy),\n",
    "            'precision': float(fold_result.precision),\n",
    "            'recall': float(fold_result.recall),\n",
    "            'conf_matrix': fold_result.conf_matrix,\n",
    "            'train_time': float(fold_result.train_time),\n",
    "            'pred_time': float(fold_result.pred_time),\n",
    "            'train_samples': fold_result.train_samples,\n",
    "            'test_samples': fold_result.test_samples,\n",
    "            'y_true': (fold_result.y_true.tolist() if hasattr(fold_result.y_true, 'tolist') else list(fold_result.y_true)) if fold_result.y_true is not None else None,\n",
    "            'y_pred': (fold_result.y_pred.tolist() if hasattr(fold_result.y_pred, 'tolist') else list(fold_result.y_pred)) if fold_result.y_pred is not None else None\n",
    "        }\n",
    "        existing_fold_details.append(new_fold_detail)\n",
    "        \n",
    "        # Reconstruct FoldResults objects from all folds\n",
    "        all_fold_results = []\n",
    "        for fd in existing_fold_details:\n",
    "            all_fold_results.append(FoldResults(\n",
    "                fold_num=fd['fold_num'],\n",
    "                f2_score=fd['f2_score'],\n",
    "                f1_score=fd['f1_score'],\n",
    "                accuracy=fd['accuracy'],\n",
    "                precision=fd['precision'],\n",
    "                recall=fd['recall'],\n",
    "                conf_matrix=fd['conf_matrix'],\n",
    "                train_time=fd['train_time'],\n",
    "                pred_time=fd['pred_time'],\n",
    "                train_samples=fd['train_samples'],\n",
    "                test_samples=fd['test_samples'],\n",
    "                y_true=np.array(fd['y_true']) if fd['y_true'] is not None else None,\n",
    "                y_pred=np.array(fd['y_pred']) if fd['y_pred'] is not None else None\n",
    "            ))\n",
    "        \n",
    "        # Extract all scores\n",
    "        all_f2_scores = [fr.f2_score for fr in all_fold_results]\n",
    "        all_f1_scores = [fr.f1_score for fr in all_fold_results]\n",
    "        all_accuracy_scores = [fr.accuracy for fr in all_fold_results]\n",
    "        all_precision_scores = [fr.precision for fr in all_fold_results]\n",
    "        all_recall_scores = [fr.recall for fr in all_fold_results]\n",
    "        \n",
    "        # Create ModelValidationResults with all folds\n",
    "        rf_validation_results = ModelValidationResults(\n",
    "            model_name='RandomForest',\n",
    "            hyperparameters=rf_params,\n",
    "            fold_results=all_fold_results,\n",
    "            f2_scores=all_f2_scores,\n",
    "            precision_scores=all_precision_scores,\n",
    "            recall_scores=all_recall_scores,\n",
    "            f1_scores=all_f1_scores,\n",
    "            accuracy_scores=all_accuracy_scores\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated existing results. Total folds validated: {len(all_fold_results)}/{N_FOLDS}\")\n",
    "    else:\n",
    "        # Create new results with just this fold\n",
    "        rf_validation_results = ModelValidationResults(\n",
    "            model_name='RandomForest',\n",
    "            hyperparameters=rf_params,\n",
    "            fold_results=[fold_result],\n",
    "            f2_scores=[f2],\n",
    "            precision_scores=[precision],\n",
    "            recall_scores=[recall],\n",
    "            f1_scores=[f1],\n",
    "            accuracy_scores=[accuracy]\n",
    "        )\n",
    "        print(f\"\\nCreated new results file. Folds validated: 1/{N_FOLDS}\")\n",
    "    \n",
    "    # Save results to file (using fixed filename so all folds save to same file)\n",
    "    rf_validation_results.save_to_file(filename)\n",
    "    \n",
    "    # Print summary if we have multiple folds\n",
    "    if len(rf_validation_results.fold_results) > 1:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CURRENT Random Forest CROSS-VALIDATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Folds completed: {len(rf_validation_results.fold_results)}/{N_FOLDS}\")\n",
    "        print(f\"Mean F2-Score: {rf_validation_results.mean_f2:.4f} ± {rf_validation_results.std_f2:.4f}\")\n",
    "        print(f\"Mean F1-Score: {rf_validation_results.mean_f1:.4f} ± {rf_validation_results.std_f1:.4f}\")\n",
    "        print(f\"Mean Accuracy: {rf_validation_results.mean_accuracy:.4f} ± {rf_validation_results.std_accuracy:.4f}\")\n",
    "        print(f\"Mean Precision: {rf_validation_results.mean_precision:.4f}\")\n",
    "        print(f\"Mean Recall: {rf_validation_results.mean_recall:.4f}\")\n",
    "        print(f\"\\nF2-Scores per fold: {[f'{s:.4f}' for s in rf_validation_results.f2_scores]}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1} validation completed. Results saved to {filepath}\")\n",
    "    return rf_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0d42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually determine the fold to validate\n",
    "# Change this variable to the fold index you want to validate (0-9 for 10 folds)\n",
    "# fold_idx is 0-based: 0 = fold 1, 1 = fold 2, ..., 9 = fold 10\n",
    "# Already processed folds: \n",
    "FOLD_TO_VALIDATE = 0  # Change this value to validate different folds\n",
    "\n",
    "# Validate the specified fold\n",
    "rf_validation_results = validate_rf_fold(FOLD_TO_VALIDATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a7bf2",
   "metadata": {},
   "source": [
    "# Neural Networks k=10 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d50a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to make one cycle of the validation of the Neural Networks model depending on the fold number pass as param\n",
    "'''\n",
    "def validate_nn_fold(fold_idx: int):\n",
    "    \"\"\"\n",
    "    Validate Neural Networks model for a single fold.\n",
    "    \n",
    "    Args:\n",
    "        fold_idx: Index of the fold to validate (0-based, 0-9 for 10 folds)\n",
    "    \n",
    "    This function:\n",
    "    - Validates the specified fold\n",
    "    - Loads existing results from file if available\n",
    "    - Updates or adds the fold result\n",
    "    - Saves all results back to file\n",
    "    \"\"\"\n",
    "\n",
    "    if fold_idx < 0 or fold_idx >= N_FOLDS:\n",
    "        raise ValueError(f\"fold_idx must be between 0 and {N_FOLDS-1}, got {fold_idx}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"Neural Networks MODEL - FOLD {fold_idx + 1} VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get Neural Network hyperparameters and sampling strategy\n",
    "    nn_params = MODEL_HYPERPARAMETERS['NeuralNetwork'].copy()\n",
    "    nn_sampling = MODEL_SAMPLING_STRATEGIES['NeuralNetwork']\n",
    "\n",
    "    print(f\"\\nHyperparameters: {nn_params}\")\n",
    "    print(f\"Sampling strategy: {nn_sampling}\")\n",
    "    \n",
    "    # Get train/test split for this fold\n",
    "    train_data, test_data = get_fold_split(ALL_FOLDS, fold_idx)\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train, y_train = split_features_target(train_data)\n",
    "    X_test, y_test = split_features_target(test_data)\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "    print(f\"Train class distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"Test class distribution: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create Neural Network model with hyperparameters\n",
    "    nn_model = MLPClassifier(**nn_params)\n",
    "    \n",
    "    # Create pipeline: StandardScaler + Neural Network (SMOTE sampling applied externally)\n",
    "    nn_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Escalado obligatorio para redes neuronales\n",
    "        ('clf', nn_model)\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    train_start = time.time()\n",
    "    nn_pipeline.fit(X_train, y_train)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_start = time.time()\n",
    "    y_pred = nn_pipeline.predict(X_test)\n",
    "    pred_time = time.time() - pred_start\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f2 = calculate_f2_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Convert confusion matrix to list format [[TN, FP], [FN, TP]]\n",
    "    cm_list = [[int(cm[0, 0]), int(cm[0, 1])], [int(cm[1, 0]), int(cm[1, 1])]]\n",
    "    \n",
    "    # Store fold results (including predictions for McNemar test)\n",
    "    fold_result = FoldResults(\n",
    "        fold_num=fold_idx + 1,\n",
    "        f2_score=f2,\n",
    "        f1_score=f1,\n",
    "        accuracy=accuracy,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        conf_matrix=cm_list,\n",
    "        train_time=train_time,\n",
    "        pred_time=pred_time,\n",
    "        train_samples=len(X_train),\n",
    "        test_samples=len(X_test),\n",
    "        y_true=y_test.values if hasattr(y_test, 'values') else np.array(y_test),\n",
    "        y_pred=y_pred\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResults for Fold {fold_idx + 1}:\")\n",
    "    print(f\"F2-Score: {f2:.4f}, F1-Score: {f1:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    print(f\"Train time: {train_time:.2f}s, Prediction time: {pred_time:.4f}s\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Load existing results or create new\n",
    "    filename = \"Neural_Networks_validation_results.json\"\n",
    "    filepath = os.path.join(STATS_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        # Load existing results\n",
    "        with open(filepath, 'r') as f:\n",
    "            existing_data = json.load(f)\n",
    "        \n",
    "        # Extract existing fold results\n",
    "        existing_fold_details = existing_data.get('fold_details', [])\n",
    "        \n",
    "        # Remove the fold if it already exists (to update it)\n",
    "        existing_fold_details = [fd for fd in existing_fold_details if fd['fold_num'] != fold_idx + 1]\n",
    "        \n",
    "        # Add the new fold result\n",
    "        new_fold_detail = {\n",
    "            'fold_num': fold_result.fold_num,\n",
    "            'f2_score': float(fold_result.f2_score),\n",
    "            'f1_score': float(fold_result.f1_score),\n",
    "            'accuracy': float(fold_result.accuracy),\n",
    "            'precision': float(fold_result.precision),\n",
    "            'recall': float(fold_result.recall),\n",
    "            'conf_matrix': fold_result.conf_matrix,\n",
    "            'train_time': float(fold_result.train_time),\n",
    "            'pred_time': float(fold_result.pred_time),\n",
    "            'train_samples': fold_result.train_samples,\n",
    "            'test_samples': fold_result.test_samples,\n",
    "            'y_true': (fold_result.y_true.tolist() if hasattr(fold_result.y_true, 'tolist') else list(fold_result.y_true)) if fold_result.y_true is not None else None,\n",
    "            'y_pred': (fold_result.y_pred.tolist() if hasattr(fold_result.y_pred, 'tolist') else list(fold_result.y_pred)) if fold_result.y_pred is not None else None\n",
    "        }\n",
    "        existing_fold_details.append(new_fold_detail)\n",
    "        \n",
    "        # Reconstruct FoldResults objects from all folds\n",
    "        all_fold_results = []\n",
    "        for fd in existing_fold_details:\n",
    "            all_fold_results.append(FoldResults(\n",
    "                fold_num=fd['fold_num'],\n",
    "                f2_score=fd['f2_score'],\n",
    "                f1_score=fd['f1_score'],\n",
    "                accuracy=fd['accuracy'],\n",
    "                precision=fd['precision'],\n",
    "                recall=fd['recall'],\n",
    "                conf_matrix=fd['conf_matrix'],\n",
    "                train_time=fd['train_time'],\n",
    "                pred_time=fd['pred_time'],\n",
    "                train_samples=fd['train_samples'],\n",
    "                test_samples=fd['test_samples'],\n",
    "                y_true=np.array(fd['y_true']) if fd['y_true'] is not None else None,\n",
    "                y_pred=np.array(fd['y_pred']) if fd['y_pred'] is not None else None\n",
    "            ))\n",
    "        \n",
    "        # Extract all scores\n",
    "        all_f2_scores = [fr.f2_score for fr in all_fold_results]\n",
    "        all_f1_scores = [fr.f1_score for fr in all_fold_results]\n",
    "        all_accuracy_scores = [fr.accuracy for fr in all_fold_results]\n",
    "        all_precision_scores = [fr.precision for fr in all_fold_results]\n",
    "        all_recall_scores = [fr.recall for fr in all_fold_results]\n",
    "        \n",
    "        # Create ModelValidationResults with all folds\n",
    "        nn_validation_results = ModelValidationResults(\n",
    "            model_name='NeuralNetwork',\n",
    "            hyperparameters=nn_params,\n",
    "            fold_results=all_fold_results,\n",
    "            f2_scores=all_f2_scores,\n",
    "            precision_scores=all_precision_scores,\n",
    "            recall_scores=all_recall_scores,\n",
    "            f1_scores=all_f1_scores,\n",
    "            accuracy_scores=all_accuracy_scores\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated existing results. Total folds validated: {len(all_fold_results)}/{N_FOLDS}\")\n",
    "    else:\n",
    "        # Create new results with just this fold\n",
    "        nn_validation_results = ModelValidationResults(\n",
    "            model_name='NeuralNetwork',\n",
    "            hyperparameters=nn_params,\n",
    "            fold_results=[fold_result],\n",
    "            f2_scores=[f2],\n",
    "            precision_scores=[precision],\n",
    "            recall_scores=[recall],\n",
    "            f1_scores=[f1],\n",
    "            accuracy_scores=[accuracy]\n",
    "        )\n",
    "        print(f\"\\nCreated new results file. Folds validated: 1/{N_FOLDS}\")\n",
    "    \n",
    "    # Save results to file (using fixed filename so all folds save to same file)\n",
    "    nn_validation_results.save_to_file(filename)\n",
    "    \n",
    "    # Print summary if we have multiple folds\n",
    "    if len(nn_validation_results.fold_results) > 1:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CURRENT Neural Networks CROSS-VALIDATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Folds completed: {len(nn_validation_results.fold_results)}/{N_FOLDS}\")\n",
    "        print(f\"Mean F2-Score: {nn_validation_results.mean_f2:.4f} ± {nn_validation_results.std_f2:.4f}\")\n",
    "        print(f\"Mean F1-Score: {nn_validation_results.mean_f1:.4f} ± {nn_validation_results.std_f1:.4f}\")\n",
    "        print(f\"Mean Accuracy: {nn_validation_results.mean_accuracy:.4f} ± {nn_validation_results.std_accuracy:.4f}\")\n",
    "        print(f\"Mean Precision: {nn_validation_results.mean_precision:.4f}\")\n",
    "        print(f\"Mean Recall: {nn_validation_results.mean_recall:.4f}\")\n",
    "        print(f\"\\nF2-Scores per fold: {[f'{s:.4f}' for s in nn_validation_results.f2_scores]}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1} validation completed. Results saved to {filepath}\")\n",
    "    return nn_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ffce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually determine the fold to validate\n",
    "# Change this variable to the fold index you want to validate (0-9 for 10 folds)\n",
    "# fold_idx is 0-based: 0 = fold 1, 1 = fold 2, ..., 9 = fold 10\n",
    "# Already processed folds: \n",
    "FOLD_TO_VALIDATE = 0  # Change this value to validate different folds\n",
    "\n",
    "# Validate the specified fold\n",
    "nn_validation_results = validate_nn_fold(FOLD_TO_VALIDATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733b155",
   "metadata": {},
   "source": [
    "# XGBoost k=10 Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to make one cycle of the validation of the XGBoost model depending on the fold number pass as param\n",
    "'''\n",
    "def validate_xgb_fold(fold_idx: int):\n",
    "    \"\"\"\n",
    "    Validate XGBoost model for a single fold.\n",
    "    \n",
    "    Args:\n",
    "        fold_idx: Index of the fold to validate (0-based, 0-9 for 10 folds)\n",
    "    \n",
    "    This function:\n",
    "    - Validates the specified fold\n",
    "    - Loads existing results from file if available\n",
    "    - Updates or adds the fold result\n",
    "    - Saves all results back to file\n",
    "    \"\"\"\n",
    "\n",
    "    if fold_idx < 0 or fold_idx >= N_FOLDS:\n",
    "        raise ValueError(f\"fold_idx must be between 0 and {N_FOLDS-1}, got {fold_idx}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"XGBoost MODEL - FOLD {fold_idx + 1} VALIDATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get XGBoost hyperparameters and sampling strategy\n",
    "    xgb_params = MODEL_HYPERPARAMETERS['XGBoost'].copy()\n",
    "    xgb_sampling = MODEL_SAMPLING_STRATEGIES['XGBoost']\n",
    "\n",
    "    print(f\"\\nHyperparameters: {xgb_params}\")\n",
    "    print(f\"Sampling strategy: {xgb_sampling}\")\n",
    "    \n",
    "    # Get train/test split for this fold\n",
    "    train_data, test_data = get_fold_split(ALL_FOLDS, fold_idx)\n",
    "    \n",
    "    # Split features and target\n",
    "    X_train, y_train = split_features_target(train_data)\n",
    "    X_test, y_test = split_features_target(test_data)\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "    print(f\"Train class distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"Test class distribution: {y_test.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Create XGBoost model with hyperparameters\n",
    "    # Add tree_method as used in training notebook (probl1_xgboost.ipynb)\n",
    "    xgb_model = XGBClassifier(**xgb_params)\n",
    "    \n",
    "    # XGBoost is used directly without pipeline (no scaler needed, no external sampling)\n",
    "    # Class imbalance is handled via scale_pos_weight parameter in hyperparameters\n",
    "\n",
    "    # Train the model\n",
    "    train_start = time.time()\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    train_time = time.time() - train_start\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_start = time.time()\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    pred_time = time.time() - pred_start\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f2 = calculate_f2_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Convert confusion matrix to list format [[TN, FP], [FN, TP]]\n",
    "    cm_list = [[int(cm[0, 0]), int(cm[0, 1])], [int(cm[1, 0]), int(cm[1, 1])]]\n",
    "    \n",
    "    # Store fold results (including predictions for McNemar test)\n",
    "    fold_result = FoldResults(\n",
    "        fold_num=fold_idx + 1,\n",
    "        f2_score=f2,\n",
    "        f1_score=f1,\n",
    "        accuracy=accuracy,\n",
    "        precision=precision,\n",
    "        recall=recall,\n",
    "        conf_matrix=cm_list,\n",
    "        train_time=train_time,\n",
    "        pred_time=pred_time,\n",
    "        train_samples=len(X_train),\n",
    "        test_samples=len(X_test),\n",
    "        y_true=y_test.values if hasattr(y_test, 'values') else np.array(y_test),\n",
    "        y_pred=y_pred\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nResults for Fold {fold_idx + 1}:\")\n",
    "    print(f\"F2-Score: {f2:.4f}, F1-Score: {f1:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "    print(f\"Train time: {train_time:.2f}s, Prediction time: {pred_time:.4f}s\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    # Load existing results or create new\n",
    "    filename = \"XGBoost_validation_results.json\"\n",
    "    filepath = os.path.join(STATS_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        # Load existing results\n",
    "        with open(filepath, 'r') as f:\n",
    "            existing_data = json.load(f)\n",
    "        \n",
    "        # Extract existing fold results\n",
    "        existing_fold_details = existing_data.get('fold_details', [])\n",
    "        \n",
    "        # Remove the fold if it already exists (to update it)\n",
    "        existing_fold_details = [fd for fd in existing_fold_details if fd['fold_num'] != fold_idx + 1]\n",
    "        \n",
    "        # Add the new fold result\n",
    "        new_fold_detail = {\n",
    "            'fold_num': fold_result.fold_num,\n",
    "            'f2_score': float(fold_result.f2_score),\n",
    "            'f1_score': float(fold_result.f1_score),\n",
    "            'accuracy': float(fold_result.accuracy),\n",
    "            'precision': float(fold_result.precision),\n",
    "            'recall': float(fold_result.recall),\n",
    "            'conf_matrix': fold_result.conf_matrix,\n",
    "            'train_time': float(fold_result.train_time),\n",
    "            'pred_time': float(fold_result.pred_time),\n",
    "            'train_samples': fold_result.train_samples,\n",
    "            'test_samples': fold_result.test_samples,\n",
    "            'y_true': (fold_result.y_true.tolist() if hasattr(fold_result.y_true, 'tolist') else list(fold_result.y_true)) if fold_result.y_true is not None else None,\n",
    "            'y_pred': (fold_result.y_pred.tolist() if hasattr(fold_result.y_pred, 'tolist') else list(fold_result.y_pred)) if fold_result.y_pred is not None else None\n",
    "        }\n",
    "        existing_fold_details.append(new_fold_detail)\n",
    "        \n",
    "        # Reconstruct FoldResults objects from all folds\n",
    "        all_fold_results = []\n",
    "        for fd in existing_fold_details:\n",
    "            all_fold_results.append(FoldResults(\n",
    "                fold_num=fd['fold_num'],\n",
    "                f2_score=fd['f2_score'],\n",
    "                f1_score=fd['f1_score'],\n",
    "                accuracy=fd['accuracy'],\n",
    "                precision=fd['precision'],\n",
    "                recall=fd['recall'],\n",
    "                conf_matrix=fd['conf_matrix'],\n",
    "                train_time=fd['train_time'],\n",
    "                pred_time=fd['pred_time'],\n",
    "                train_samples=fd['train_samples'],\n",
    "                test_samples=fd['test_samples'],\n",
    "                y_true=np.array(fd['y_true']) if fd['y_true'] is not None else None,\n",
    "                y_pred=np.array(fd['y_pred']) if fd['y_pred'] is not None else None\n",
    "            ))\n",
    "        \n",
    "        # Extract all scores\n",
    "        all_f2_scores = [fr.f2_score for fr in all_fold_results]\n",
    "        all_f1_scores = [fr.f1_score for fr in all_fold_results]\n",
    "        all_accuracy_scores = [fr.accuracy for fr in all_fold_results]\n",
    "        all_precision_scores = [fr.precision for fr in all_fold_results]\n",
    "        all_recall_scores = [fr.recall for fr in all_fold_results]\n",
    "        \n",
    "        # Create ModelValidationResults with all folds\n",
    "        xgb_validation_results = ModelValidationResults(\n",
    "            model_name='XGBoost',\n",
    "            hyperparameters=xgb_params,\n",
    "            fold_results=all_fold_results,\n",
    "            f2_scores=all_f2_scores,\n",
    "            precision_scores=all_precision_scores,\n",
    "            recall_scores=all_recall_scores,\n",
    "            f1_scores=all_f1_scores,\n",
    "            accuracy_scores=all_accuracy_scores\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nUpdated existing results. Total folds validated: {len(all_fold_results)}/{N_FOLDS}\")\n",
    "    else:\n",
    "        # Create new results with just this fold\n",
    "        xgb_validation_results = ModelValidationResults(\n",
    "            model_name='XGBoost',\n",
    "            hyperparameters=xgb_params,\n",
    "            fold_results=[fold_result],\n",
    "            f2_scores=[f2],\n",
    "            precision_scores=[precision],\n",
    "            recall_scores=[recall],\n",
    "            f1_scores=[f1],\n",
    "            accuracy_scores=[accuracy]\n",
    "        )\n",
    "        print(f\"\\nCreated new results file. Folds validated: 1/{N_FOLDS}\")\n",
    "    \n",
    "    # Save results to file (using fixed filename so all folds save to same file)\n",
    "    xgb_validation_results.save_to_file(filename)\n",
    "    \n",
    "    # Print summary if we have multiple folds\n",
    "    if len(xgb_validation_results.fold_results) > 1:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"CURRENT XGBoost CROSS-VALIDATION SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Folds completed: {len(xgb_validation_results.fold_results)}/{N_FOLDS}\")\n",
    "        print(f\"Mean F2-Score: {xgb_validation_results.mean_f2:.4f} ± {xgb_validation_results.std_f2:.4f}\")\n",
    "        print(f\"Mean F1-Score: {xgb_validation_results.mean_f1:.4f} ± {xgb_validation_results.std_f1:.4f}\")\n",
    "        print(f\"Mean Accuracy: {xgb_validation_results.mean_accuracy:.4f} ± {xgb_validation_results.std_accuracy:.4f}\")\n",
    "        print(f\"Mean Precision: {xgb_validation_results.mean_precision:.4f}\")\n",
    "        print(f\"Mean Recall: {xgb_validation_results.mean_recall:.4f}\")\n",
    "        print(f\"\\nF2-Scores per fold: {[f'{s:.4f}' for s in xgb_validation_results.f2_scores]}\")\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx + 1} validation completed. Results saved to {filepath}\")\n",
    "    return xgb_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually determine the fold to validate\n",
    "# Change this variable to the fold index you want to validate (0-9 for 10 folds)\n",
    "# fold_idx is 0-based: 0 = fold 1, 1 = fold 2, ..., 9 = fold 10\n",
    "# Already processed folds: \n",
    "FOLD_TO_VALIDATE = 0  # Change this value to validate different folds\n",
    "\n",
    "# Validate the specified fold\n",
    "xgb_validation_results = validate_xgb_fold(FOLD_TO_VALIDATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
